<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://blog.jhangy.us</id>
    <title>Jhangyu</title>
    <updated>2019-06-17T07:45:59.719Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="http://blog.jhangy.us"/>
    <link rel="self" href="http://blog.jhangy.us/atom.xml"/>
    <subtitle>記錄冷門的技術資訊，打擊商業公司的虛假宣傳</subtitle>
    <logo>http://blog.jhangy.us/images/avatar.png</logo>
    <icon>http://blog.jhangy.us/favicon.ico</icon>
    <rights>All rights reserved 2019, Jhangyu</rights>
    <entry>
        <title type="html"><![CDATA[Google Camera Port (GCam) 資訊匯總及Q&A]]></title>
        <id>http://blog.jhangy.us/post/google-camera-port-gcam-q&amp;a</id>
        <link href="http://blog.jhangy.us/post/google-camera-port-gcam-q&amp;a">
        </link>
        <updated>2019-06-17T04:00:51.000Z</updated>
        <content type="html"><![CDATA[<h1 id="google-camera介紹">Google Camera介紹</h1>
<p>Google Camera是目前演算法最先進的Android相機App，遠遠超過其他手機廠使用的演算法
能夠讓任何使用了Google Camera (Port)的手機在夜拍上擁有極其出色的表現</p>
<h2 id="hdr">HDR+</h2>
<p>支撐著Google Camera強大的夜拍效果的則是其被命名為「HDR+」的功能
這功能其實有點名稱誤用，在Google Camera早期3.X版本(Nexus 5時期)此功能確實是 <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">只有HDR的效果</a>
<img src="https://3.bp.blogspot.com/-utUr33f9l_Y/VE1iR-ApMkI/AAAAAAAAAZQ/N2UaHstq3Uw/s1600/HDR%2B2.png" alt="img">
透過合成多張短曝光、曝光度不足的照片，除了能為照片帶來高動態範圍的好處、同時也能透過多幀照片中的色彩資訊「平均」，將畫面中的雜訊濾除。</p>
<p><img src="https://2.bp.blogspot.com/-EQmUwsrM5Lo/VE26fr8w-sI/AAAAAAAAAZ0/nednky5nXMs/s1600/princeton-church-normal-with-inset.jpg" alt="img"></p>
<p><img src="https://1.bp.blogspot.com/-qNiuoM3vy94/VE27Ed4kMKI/AAAAAAAAAZ8/HLzCuWByzSI/s1600/princeton-church-hdrp-with-inset.jpg" alt="img"></p>
<p>而後隨著手機感光元件尺寸越來越大，手機廠是能夠購買的演算法（例如說ArcSoft）對動態範圍參數的改進，高動態範圍的照片反而越來越不是HDR+主要的特點，Google在<strong>多幀</strong>降噪算法上的各種改進，逐漸成為HDR+此功能的演算法主角。</p>
<h2 id="super-zoom">Super Zoom</h2>
<p>Google的相機算法工程師在多幀拍攝的改良上玩的出神入化，後期又在使用單鏡頭達成兩倍放大的效果，這是怎麼辦到的呢？</p>
<p>具體原理是這樣的，這是我們一般相機感光元件所使用的RGB拜耳陣列：</p>
<p><img src="https://2.bp.blogspot.com/-bsfbKQzqvB4/W8O_6vO0f0I/AAAAAAAADX0/W3rdX0hfiIwXoM2_Tfy08aHoPH3ZQyc1wCLcBGAs/s200/image14.png" alt="img"></p>
<p>你可以觀察到每一個感光像素並不是同時偵測紅綠藍三種顏色，相反的拜耳陣列則是將感光像素分成紅綠藍三組，在每一組感光像素前面加上了濾光片，讓他們只能感知特定顏色的光。聰明的你可能發現兩個問題：</p>
<ul>
<li>這樣看起來要四個單色的感光像素才能還原我們一般照片中的一個全色像素，像素不是被稀釋了嗎？</li>
<li>這些感光像素是完全間隔排列的，中間的像素空缺的像素要怎麼辦呢？</li>
</ul>
<p>第一個問題的答案是：沒錯，我們一般用的拜耳陣列CMOS像素是真的被稀釋成1/3的，所以實際解像力會不如像素量相同的Foveon結構感光元件。</p>
<p>第二個問題的答案則是：中間缺失的像素資訊，是用猜的！</p>
<p><img src="https://4.bp.blogspot.com/-E0Cobuog7Xg/W8PAJOS6CwI/AAAAAAAADX4/qRAoZLuzo4o6kf-KcmwaVm_Ji02jE_kmgCLcBGAs/s640/image1.png" alt="img"></p>
<p>目前對於拜耳陣列的CMOS結構已經有成熟的猜色算法，基本上顏色不會失真太多，但是缺點就是當鏡頭光學校正能力不足時，會造成底片時代沒有的「摩爾紋」、「偽色」、和「紫邊」。</p>
<p>講了半天CMOS，那這個SuperZoom是怎麼做到的呢？</p>
<p>還記得我們一般使用的拜耳陣列CMOS像素量是灌水的嗎？一般相機常說的2400萬像素實際並不是2400萬像素，而是僅有800萬像素。而這個時候如果我們讓CMOS的連拍三幢讓像素位移，讓每個像素都分別對紅、綠、藍三個顏色進行感光，不就擁有完整的2400萬像素了嗎？</p>
<p><img src="https://3.bp.blogspot.com/-SRJfJyQcNRk/W8PAjDvVtdI/AAAAAAAADYE/tUfWIsw5HUEl3IXrLadqUPsMXdGWhSNIACLcBGAs/s640/image13.png" alt="img"></p>
<p>而現在透過像素位移取得完整的2400萬像素的照片，其實就等同一般狀態下的4800萬畫素，也就是我們可以直接得到理論上清晰度相較傳統方法高兩倍的照片，足夠我們將他放大兩倍使用了。</p>
]]></content>
    </entry>
</feed>